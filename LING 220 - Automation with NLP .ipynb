{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "\n",
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopword removal \n",
    "\n",
    "Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words . The general strategy for determining a stop list is to sort the terms by collection frequency (the total number of times each term appears in the document collection), and then to take the most frequent terms, often hand-filtered for their semantic content relative to the domain of the documents being indexed, as a stop list , the members of which are then discarded during indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sometimes, extremely common words would appear little value helping select documents matching user need excluded vocabulary entirely. These words called stop words . The general "
     ]
    }
   ],
   "source": [
    "sentence = '''Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words . The general strategy for determining a stop list is to sort the terms by collection frequency (the total number of times each term appears in the document collection), and then to take the most frequent terms, often hand-filtered for their semantic content relative to the domain of the documents being indexed, as a stop list , the members of which are then discarded during indexing'''\n",
    "word_list = sentence.split()\n",
    "\n",
    "filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "for f in filtered_words[:25]:\n",
    "    print(f,end=' ') #first 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some which to be of in a are from the are for a is to the by of each in the and then to the "
     ]
    }
   ],
   "source": [
    "stopwords_found = [word for word in word_list if word in stopwords.words('english')]\n",
    "for f in stopwords_found[:25]:\n",
    "    print(f,end=' ') #first 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntex - part of speach tagging  \n",
    "\n",
    "* [good SO post](https://stackoverflow.com/a/30823202/5728614) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def tag_words(text):\n",
    "\n",
    "    if type(text)== str:\n",
    "        text = word_tokenize(text)\n",
    "        \n",
    "    tags_pos_tag = pos_tag(text)\n",
    "    \n",
    "    treebankTagger = nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')\n",
    "    tags_maxent_treebank = treebankTagger.tag(text)\n",
    " \n",
    "    print( '{:<20s} {:<15s} {:<10s}'.format(\"word\", \"default tagger\", \"maxent treebank tagger\"))\n",
    "    for i in range(len(tags_pos_tag)):      \n",
    "        print( '{:<20s} {:<15s} {:<10s}'.format(tags_pos_tag[i][0], tags_pos_tag[i][1],tags_maxent_treebank[i][1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that `text.split() == word_tokenize(text)`\n",
    "> True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                 default tagger  maxent treebank tagger\n",
      "The                  DT              DT        \n",
      "quick                JJ              NN        \n",
      "brown                NN              NN        \n",
      "fox                  NN              NN        \n",
      "jumps                VBZ             NNS       \n",
      "over                 IN              IN        \n",
      "the                  DT              DT        \n",
      "lazy                 JJ              NN        \n",
      "dog                  NN              NN        \n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tag_words(text)# returns void "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphology - stemming\n",
    "* [stem](https://pythonprogramming.net/stemming-nltk-tutorial/)\n",
    "* [affix](https://stackoverflow.com/questions/55425525/can-the-porter-stemmer-return-the-affix-rather-than-the-stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example words:  ['python', 'pythoner', 'pythoning', 'pythoned', 'pythonly']\n",
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "print('example words: ', example_words)\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gracefully full of force for the sake of being forcefully full of grace \n",
      "becomes,\n",
      "grace full of forc for the sake of be forc full of grace \n",
      "------------------\n",
      "recharging recharg charging builderhood builder preclassified preclass duckhunter \n",
      "becomes,\n",
      "recharg recharg charg builderhood builder preclassifi preclass duckhunt "
     ]
    }
   ],
   "source": [
    "text = \"gracefully full of force for the sake of being forcefully full of grace\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(text,'\\nbecomes,')\n",
    "for w in words:\n",
    "    print(ps.stem(w), end = ' ')\n",
    "\n",
    "print(\"\\n------------------\")\n",
    "text = \"recharging recharg charging builderhood builder preclassified preclass duckhunter\"\n",
    "words = word_tokenize(text)\n",
    "print(text,'\\nbecomes,')\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w), end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                 default tagger  maxent treebank tagger\n",
      "recharging           VBG             VBG       \n",
      "recharg              NN              NN        \n",
      "charging             VBG             VBG       \n",
      "builderhood          NN              NN        \n",
      "builder              NN              NN        \n",
      "preclassified        VBD             VBD       \n",
      "preclass             NN              NN        \n",
      "duckhunter           NN              NN        \n"
     ]
    }
   ],
   "source": [
    "tag_words(text.replace('.',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging non-stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                 default tagger  maxent treebank tagger\n",
      "recharg              NN              NN        \n",
      "recharg              NN              NN        \n",
      "charg                NN              NN        \n",
      "builderhood          NN              NN        \n",
      "builder              NN              NN        \n",
      "preclassifi          NN              NN        \n",
      "preclass             NN              NN        \n",
      "duckhunt             NN              NN        \n"
     ]
    }
   ],
   "source": [
    "text_stemmed = [ps.stem(w) for w in words if w != '.'] # or .replace('.',''); IDK which is faster... \n",
    "tag_words(text_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gettign affixes (conversely, removing the stem)  \n",
    "#### Marginal \"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursion(word):\n",
    "    stem = ps.stem(word)\n",
    "    print('word: ', word)\n",
    "    print('Stem: ', stem)\n",
    "    if(word.replace(stem,'') != word):\n",
    "        print('Affix:', word.replace(stem,''))\n",
    "    else: \n",
    "        print(\"*fail or done*\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    if stem != word:\n",
    "        recursion(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  eating\n",
      "Stem:  eat\n",
      "Affix: ing\n",
      "\n",
      "\n",
      "word:  eat\n",
      "Stem:  eat\n",
      "Affix: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word = 'eating'\n",
    "recursion(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  recharging\n",
      "Stem:  recharg\n",
      "Affix: ing\n",
      "\n",
      "\n",
      "word:  recharg\n",
      "Stem:  recharg\n",
      "Affix: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word = 'recharging'\n",
    "recursion(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  preclassified\n",
      "Stem:  preclassifi\n",
      "Affix: ed\n",
      "\n",
      "\n",
      "word:  preclassifi\n",
      "Stem:  preclassifi\n",
      "Affix: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word = 'preclassified'\n",
    "recursion(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonology - environment finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentance   pot bot mop fought cot on top of octopus ʌ \n",
      "Current target is:  o\n",
      "\n",
      "p_t\n",
      "b_t\n",
      "m_p\n",
      "f_u\n",
      "c_t\n",
      "#_n\n",
      "t_p\n",
      "#_f\n",
      "#_c\n",
      "t_p\n",
      "Current target is:  a\n",
      "\n",
      "Current target is:  p\n",
      "\n",
      "#_o\n",
      "o_#\n",
      "o_#\n",
      "o_u\n",
      "Current target is:  t\n",
      "\n",
      "o_#\n",
      "o_#\n",
      "h_#\n",
      "o_#\n",
      "#_o\n",
      "c_o\n",
      "Current target is:  ʌ\n",
      "\n",
      "#_#\n"
     ]
    }
   ],
   "source": [
    "targets = ['o','a', 'p', 't', 'ʌ']\n",
    "word_list = \"pot bot mop fought cot on top of octopus ʌ\"\n",
    "\n",
    "word_list = ' ' + word_list + ' '\n",
    "\n",
    "print('Sentance ',word_list)\n",
    "\n",
    "word_list = word_list.replace(\" \", \"#\")\n",
    "for target in targets:\n",
    "    print('Current target is: ',target,end ='\\n\\n')\n",
    "    \n",
    "    word_list_current = word_list.replace(target, \"_\")\n",
    "    \n",
    "    for i in range(len(word_list)):\n",
    "        if(word_list_current[i] == '_'):\n",
    "            print(word_list_current[i-1:i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/49269085/5728614\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
